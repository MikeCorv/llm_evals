{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNvUvMw2gPoaM6nYgcYmDIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeCorv/llm_evals/blob/main/MMLU_TEST_QWEN2_5B_LLAMA3B_1_5SHOTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "c7zZFoKy-wl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lm_eval[vllm,gpu] accelerate\n"
      ],
      "metadata": {
        "id": "sDOrwDv-IVJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "1Sf8jOOQMl0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHGsYTwfIRBD"
      },
      "outputs": [],
      "source": [
        "import lm_eval\n",
        "import torch\n",
        "import bitsandbytes\n",
        "import json\n",
        "import gc\n",
        "from google.colab import userdata, drive\n",
        "from huggingface_hub import login\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HF KEYS\n",
        "login(userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "605VWvv7TP7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Create a folder for your results\n",
        "save_folder = \"/content/drive/MyDrive/LLM_Results\"\n",
        "\n",
        "print(f\"üìÇ Results will be saved to: {save_folder}\")"
      ],
      "metadata": {
        "id": "aDVAJhUJq6Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_name = \"mmlu\""
      ],
      "metadata": {
        "id": "q7S1mEabI3X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "eypWs8YxMDxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shots_list = [x for x in range (6)]\n",
        "print(shots_list)"
      ],
      "metadata": {
        "id": "Mt8IK--2pQvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id_1 = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "qwen_scores = []"
      ],
      "metadata": {
        "id": "dzKVu0mtrZsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_test = lm_eval.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=f\"pretrained={model_id_1},load_in_4bit=True,trust_remote_code=True\",\n",
        "      tasks=[task_name],\n",
        "      num_fewshot=2,\n",
        "      limit = 10,\n",
        "      verbosity = \"WARNING\",\n",
        "      apply_chat_template=True,\n",
        "      fewshot_as_multiturn=True,\n",
        "      log_samples = True,\n",
        "      batch_size = 8\n",
        "  )"
      ],
      "metadata": {
        "id": "eRbVAJZ0j2_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in results_test.keys():\n",
        "  print(key)"
      ],
      "metadata": {
        "id": "P8J8Y8bgreaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(results_test['samples'], indent=4))"
      ],
      "metadata": {
        "id": "XihwTZKWe1Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for shot_num in shots_list:\n",
        "  results = lm_eval.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=f\"pretrained={model_id_1},load_in_4bit=True,trust_remote_code=True\",\n",
        "      tasks=[task_name],\n",
        "      num_fewshot=shot_num,\n",
        "      limit = 10,\n",
        "      verbosity = \"WARNING\"\n",
        "  )\n",
        "\n",
        "\n",
        "  entry = {\n",
        "            \"model_id\": model_id_1,\n",
        "            \"shots\": shot_num,\n",
        "            \"results\": json.dumps(results['results'], indent = 2) # This contains the total MMLU + all subtasks\n",
        "        }\n",
        "  qwen_scores.append(entry)\n",
        "\n"
      ],
      "metadata": {
        "id": "cXem24eBJt6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file = f\"{save_folder}/qwen_scores_0_to_5.json\"\n",
        "\n",
        "# Save the list to JSON\n",
        "with open(save_file, \"w\") as f:\n",
        "    json.dump(qwen_scores, f, indent=4)\n",
        "\n",
        "print(f\"üíæ Success! Data saved to: {save_file}\")"
      ],
      "metadata": {
        "id": "MfRASn8W0SCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "LKjULCcfUdlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"‚úÖ Success! Logged in as: {user_info['name']}\")\n",
        "    print(f\"‚úÖ Token permissions: {user_info['auth']['accessToken']['role']}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Still failing. Error:\", e)"
      ],
      "metadata": {
        "id": "SN5yMTZmV6G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id_2 = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "llama_scores = []"
      ],
      "metadata": {
        "id": "Ss8WrXY-eflH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for shot_num in shots_list:\n",
        "  results = lm_eval.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=f\"pretrained={model_id_2},load_in_4bit=True,trust_remote_code=True\",\n",
        "      tasks=[task_name],\n",
        "      num_fewshot=shot_num,\n",
        "      limit = 10,\n",
        "      verbosity = \"WARNING\",\n",
        "      batch_size = \"auto\"\n",
        "\n",
        "  )\n",
        "  entry = {\n",
        "            \"model_id\": model_id_2,\n",
        "            \"shots\": shot_num,\n",
        "            \"results\": json.dumps(results['results'], indent = 2) # This contains the total MMLU + all subtasks\n",
        "        }\n",
        "  llama_scores.append(entry)"
      ],
      "metadata": {
        "id": "-Tjrl1nIR9aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llama_scores)"
      ],
      "metadata": {
        "id": "hIT51x94jJFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_file_2 = f\"{save_folder}/llama_scores_0_to_5.json\"\n",
        "\n",
        "# Save the list to JSON\n",
        "with open(save_file_2, \"w\") as f:\n",
        "    json.dump(llama_scores, f, indent=4)\n",
        "\n",
        "print(f\"üíæ Success! Data saved to: {save_file_2}\")"
      ],
      "metadata": {
        "id": "xwWdTS59jPyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_llama_json = json.dumps(results_Llama32_1B['results'], indent = 2)\n",
        "print(results_llama_json)"
      ],
      "metadata": {
        "id": "oDirENx-ftjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}